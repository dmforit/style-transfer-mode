| Название | Год | Авторы | Ссылка | Краткое содержание |
|---|---|---|---|---|
| Image Style Transfer Using Convolutional Neural Networks | 2016 | Leon A. Gatys, Alexander S. Ecker, Matthias Bethge | [link](http://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html) | Впервые представлена концепция передачи стиля изображений с использованием сверточных нейронных сетей, позволяющая раздельно моделировать содержание и стиль для создания художественных изображений. |
| Perceptual Losses for Real-Time Style Transfer and Super-Resolution | 2016 | Justin Johnson, Alexandre Alahi, Li Fei-Fei | [link](https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43) | Предложен метод быстрого переноса стиля путем обучения сверточных нейронных сетей с использованием перцепционных потерь, что позволяет осуществлять стилизацию изображений в реальном времени. |
| Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization | 2017 | Xun Huang, Serge Belongie | [link](http://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html) | Введена адаптивная нормализация инстансов (AdaIN), позволяющая эффективно переносить произвольные стили на изображения в реальном времени путем выравнивания статистик признаков между контентом и стилем. |
| Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks | 2017 | Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros | [link](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) | Представлен CycleGAN, позволяющий осуществлять перенос стиля между несопоставленными наборами данных, сохраняя структуру контента через циклическую согласованность. |
| Image-to-Image Translation with Conditional Adversarial Networks | 2017 | Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros | [link](http://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html) | Введен Pix2Pix, общий подход для задач преобразования изображений с использованием условных GAN, обученных на парных данных. |
| Instance Normalization: The Missing Ingredient for Fast Stylization | 2016 | Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky | [link](https://arxiv.org/abs/1607.08022) | Представлена нормализация инстансов как улучшение для генеративных моделей, что приводит к более быстрой и качественной стилизации изображений. |
| Universal Style Transfer via Feature Transforms | 2017 | Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, Jan Kautz | [link](https://proceedings.neurips.cc/paper/2017/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html) | Предложен метод переноса стиля без обучения, основанный на выравнивании статистик признаков между контентом и стилем. |
| Deep Photo Style Transfer | 2017 | Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala | [link](http://openaccess.thecvf.com/content_cvpr_2017/html/Luan_Deep_Photo_Style_CVPR_2017_paper.html) | Разработан метод фотореалистичного переноса стиля между фотографиями с сохранением деталей и структур исходного изображения. |
| Demystifying Neural Style Transfer | 2017 | Yingying Li, Ning Wang, Jing Liu, Xiaofei He | [link](https://arxiv.org/abs/1701.01036) | Проведен анализ и обобщение методов передачи стиля, предложены улучшения для повышения качества стилизации. |
| A Closed-Form Solution to Photorealistic Image Stylization | 2018 | Yijun Li, Ming-Yu Liu, Jun-Yan Zhu, Ting-Chun Wang, Alexei A. Efros | [link](http://openaccess.thecvf.com/content_ECCV_2018/html/Yijun_Li_A_Closed-form_Solution_ECCV_2018_paper.html) | Предложен эффективный алгоритм для фотореалистичного переноса стиля с использованием линейных преобразований в пространстве признаков. |
| Avatar-Net: Multi-Scale Zero-Shot Style Transfer by Feature Decoration | 2018 | Lu Sheng, Zihang Lin, Jing Shao, Xiaogang Wang | [link](http://openaccess.thecvf.com/content_cvpr_2018/html/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.html) | Представлен метод для нулевого переноса стиля без обучения на конкретных парах изображений, используя декорирование признаков на разных масштабах. |
| Artistic Style Transfer for Videos | 2016 | Manuel Ruder, Alexey Dosovitskiy, Thomas Brox | [link](https://link.springer.com/chapter/10.1007/978-3-319-45886-1_3) | Разработан метод переноса стиля на видео с учетом временной согласованности, что устраняет мерцание и обеспечивает плавность стилизации.  |
| StyleBank: An Explicit Representation for Neural Image Style Transfer | 2017 | Dongdong Chen, Lu Yuan, Jing Liao, Nanyun Peng, Fang Wen, Gang Hua, Sing Bing Kang | [link](http://openaccess.thecvf.com/content_cvpr_2017/html/Chen_StyleBank_An_Explicit_CVPR_2017_paper.html) | Введена концепция StyleBank — набора фильтров, каждый из которых представляет определенный стиль, что позволяет эффективно выполнять многостилевую стилизацию изображений. |
| Style Transfer by Relaxed Optimal Transport and Self-Similarity | 2019 | Nicholas P. Kolkin, Jason Salavon, Gregory Shakhnarovich | [link](http://openaccess.thecvf.com/content_CVPR_2019/html/Kolkin_Style_Transfer_by_Relaxed_Optimal_Transport_and_Self-Similarity_CVPR_2019_paper.html) | Представлен метод, использующий оптимальный транспорт и самоподобие для более точного и контролируемого переноса стиля. |
| Style Transfer from Non-Parallel Text by Cross-Alignment | 2017 | Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola | [link](https://proceedings.neurips.cc/paper_files/paper/2017/hash/2d2c8394e31101a261abf1784302bf75-Abstract.html) | Предложен метод переноса стиля в текстах без параллельных корпусов, разделяя содержание и стиль через перекрестное выравнивание. |
| Learning Linear Transformations for Fast Image and Video Style Transfer | 2019 | Yijun Li, Ming-Yu Liu, Xueting Li, Ming-Hsuan Yang, Jan Kautz | [link](http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_Linear_Transformations_for_Fast_Image_and_Video_Style_Transfer_CVPR_2019_paper.html) | Разработан быстрый метод переноса стиля с использованием обученных линейных преобразований, применимых как к изображениям, так и к видео. |
| Photorealistic Style Transfer via Wavelet Transforms | 2019 | Yunjoon Yoo, Youngjung Uh, Sanghyun Chun, Byeongju Kang, Jong Chul Ye | [link](http://openaccess.thecvf.com/content_ICCV_2019/html/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.html) | Предложен метод, использующий вейвлет-преобразования для сохранения деталей при фотореалистичном переносе стиля. |
| Neural Style Transfer: A Review | 2019 | Yanpei Jing, Yong Xu, Zongsheng Feng, Jun Yu, Yao Zhao | [link](https://ieeexplore.ieee.org/abstract/document/8732370/) | Предоставлен всеобъемлющий обзор методов нейронного переноса стиля, обсуждены достижения и направления будущих исследований. |
| Inversion-Based Style Transfer with Diffusion Models | 2023 | Nianju Zhang, Fanfan Huang, Hao Tang, Xiaoping Huang | [link](http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.html) | Введен новый подход к переносу стиля, объединяющий диффузионные модели и методы инверсии для достижения высококачественной стилизации. |
| CLIPstyler: Image Style Transfer with a Single Text Condition | 2022 | Jisoo Kwon, Jong Chul Ye | [link](http://openaccess.thecvf.com/content/CVPR2022/html/Kwon_CLIPstyler_Image_Style_Transfer_With_a_Single_Text_Condition_CVPR_2022_paper.html) | Представлен метод, использующий модель CLIP для текстово-ориентированного переноса стиля, позволяя стилизовать изображения на основе текстовых описаний. |
| StyTr^2: Image Style Transfer with Transformers | 2022 | Fengnian Deng, Weihao Yuan, Wenjing Dong, Hao Huang, Chong Ma, Xin Yang | [link](http://openaccess.thecvf.com/content/CVPR2022/html/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.html) | Предложен метод переноса стиля, использующий трансформеры для моделирования дальнодействующих зависимостей, достигая высококачественных результатов стилизации. |
| ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows | 2021 | Shuyang An, Yu-Kun Lai, Ying Song, Dejing Dou | [link](http://openaccess.thecvf.com/content/CVPR2021/html/An_ArtFlow_Unbiased_Image_Style_Transfer_via_Reversible_Neural_Flows_CVPR_2021_paper.html) | Представлен метод на основе обратимых нейронных потоков для беспристрастного переноса стиля, который сохраняет структуру контента без искажений. |
| AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer | 2021 | Tongtong Liu, Tianyi Lin, Dongliang He, Fu Li, Mingming Wang, Xin Li, Errui Ding | [link](http://openaccess.thecvf.com/content/ICCV2021/html/Liu_AdaAttN_Revisit_Attention_Mechanism_in_Arbitrary_Neural_Style_Transfer_ICCV_2021_paper.html) | Представлен модуль на основе внимания для переноса стиля, улучшающий качество стилизации путем лучшего моделирования корреляций между контентом и стилем. |
| StyleFormer: Real-Time Arbitrary Style Transfer via Parametric Style Composition | 2021 | Zhecheng Wu, Lingzhi Hu, Linsheng Sheng, Dong Xu | [link](https://openaccess.thecvf.com/content/ICCV2021/html/Wu_StyleFormer_Real-Time_Arbitrary_Style_Transfer_via_Parametric_Style_Composition_ICCV_2021_paper.html) | Предложена архитектура на основе трансформеров для реального времени произвольного переноса стиля, позволяющая эффективно смешивать и интерполировать стили. |
| AesUST: Towards Aesthetic-Enhanced Universal Style Transfer | 2022 | Zhe Wang, Zhen Zhang, Lizhen Zhao, Zhijing Zuo, Ailin Li, Weisheng Xing | [link](https://dl.acm.org/doi/abs/10.1145/3503161.3547939) | Введен метод эстетически улучшенного переноса стиля, который включает оценку эстетики для создания более визуально привлекательных результатов. |
| DualAST: Dual Style-Learning Networks for Artistic Style Transfer | 2021 | Liang Chen, Lizhen Zhao, Zhe Wang, Haofei Zhang, Zhijing Zuo, Ailin Li, Weisheng Xing | [link](http://openaccess.thecvf.com/content/CVPR2021/html/Chen_DualAST_Dual_Style-Learning_Networks_for_Artistic_Style_Transfer_CVPR_2021_paper.html) | Предложена двухсеточная архитектура, которая отдельно изучает и комбинирует глобальные и локальные стили для более нюансированного художественного переноса стиля. |
| Consistent Style Transfer | 2022 | Zhiming Luo, Zhizhong Han, Lingxiao Yang, Le Zhang | [link](https://arxiv.org/abs/2201.02233) |  Разработан метод для обеспечения согласованности стиля в разных регионах изображения, улучшая визуальную целостность стилизованного результата.  |
| QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity | 2023 | Junshu Huang, Dongdong An, Weiwei Wei, Jiawei Luo, Zheng Qin | [link](http://openaccess.thecvf.com/content/CVPR2023/html/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.html) | Представлен метод квантования для переноса стиля, который повышает визуальную точность и уменьшает артефакты в стилизованных изображениях. |
| StyDiffusion: Controllable Disentangled Style Transfer via Diffusion Models | 2023 | Li Wang, Lizhen Zhao, Weisheng Xing | [link](http://openaccess.thecvf.com/content/ICCV2023/html/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.html) | Введен подход на основе диффузионных моделей для переноса стиля, позволяющий контролируемо и раздельно манипулировать стилевыми и контентными признаками. |
| StyleFlow: Attribute-Conditioned Exploration of StyleGAN-Generated Images | 2020 | Rameen Abdal, Yipeng Qin, Peter Wonka | [link](http://openaccess.thecvf.com/content_CVPR_2020/html/Abdal_StyleFlow_Attribute-Conditioned_Exploration_of_StyleGAN-Generated_Images_CVPR_2020_paper.html) | Представлен метод навигации по латентному пространству StyleGAN с использованием нормализующих потоков, позволяющий контролируемую генерацию и редактирование изображений на основе атрибутов. |
| StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks | 2019 | Tero Karras, Samuli Laine, Timo Aila | [link](http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html) | Введен StyleGAN — архитектура генератора, которая управляет процессом синтеза через стилевой подход, обеспечивая высококачественную генерацию изображений и возможности смешивания стилей. |
| Neural Preset for Color Style Transfer | 2023 | Yujia Ke, Yi Liu, Lianling Zhu, Ning Zhao | [link](http://openaccess.thecvf.com/content/CVPR2023/html/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.html) | Предложена нейронная сеть, которая обучает цветовые трансформационные пресеты для переноса стиля, позволяя эффективно и последовательно изменять цветовой стиль изображений. |
| StyleFlow: Unsupervised Representation Disentanglement in StyleGAN | 2020 | Rameen Abdal, Yipeng Qin, Peter Wonka | [link](https://arxiv.org/abs/2008.02401) | Исследовано несупервизорное разбиение латентных представлений в StyleGAN с использованием условных непрерывных нормализующих потоков, улучшая контролируемость в синтезе изображений. |
| DiffStyle: Controllable Dual Diffusion for Text-Driven Image Stylization | 2023 | Yao Huang, Zhipeng Zhang, Feng Tang, Chong Ma | [link](https://ieeexplore.ieee.org/abstract/document/10387416/) | Введен подход на основе двойных диффузионных моделей для текстово-ориентированной стилизации изображений, позволяющий тонко контролировать процесс стилизации, управляемый текстовыми описаниями. |
| Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes | 2021 | Andrey Kotovenko, Marc-André Gardner, Matthias Grundmann, Daniel Cohen-Or, Luc Van Gool | [link](http://openaccess.thecvf.com/content/CVPR2021/html/Kotovenko_Rethinking_Style_Transfer_From_Pixels_to_Parameterized_Brushstrokes_CVPR_2021_paper.html) | Предложен метод, представляющий перенос стиля в виде параметризованных мазков кисти, выходя за рамки пиксельных методов для более естественной и интерпретируемой стилизации. |
| Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer | 2023 | Hyeonjun Yang, Gihyun Hwang, Jong Chul Ye | [link](http://openaccess.thecvf.com/content/ICCV2023/html/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.html) | Представлен подход нулевого обучения с использованием контрастной потери в диффузионных моделях для текстово-ориентированного переноса стиля, позволяющий эффективно стилизовать изображения без дополнительной обучающей выборки.  |
| StylerF: Zero-Shot 3D Style Transfer of Neural Radiance Fields | 2023 | Xinyang Liu, Peng Wang, Yuan Chen, Jue Wang, Xin Yang | [link](http://openaccess.thecvf.com/content/CVPR2023/html/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.html) | Введен метод переноса стиля на 3D-сцены, представленные нейронными радиационными полями (NeRF), достигая стилизованной визуализации в режиме нулевого обучения. |
| StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing | 2023 | Jiapeng Zhu, Timo van de Weijer, Tuan-Hung Vu, Fahad Shahbaz Khan, Qi Dong | [link](https://arxiv.org/abs/2303.15649) | Предложена техника редактирования изображений путем инверсии встраиваний подсказок в диффузионных моделях, позволяющая текстовое манипулирование сгенерированными изображениями. |
| In-Context Learning Unlocked for Diffusion Models | 2023 | Yizhuo Wang, Zhiwei Jiang, Yujun Shen, Ping Luo, Daxin Jiang, Nan Duan | [link](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b3750390ca8b931fb9ca988647940cb-Abstract-Conference.html) | Исследованы возможности обучения в контексте для диффузионных моделей, улучшая их способность адаптироваться к новым стилям и шаблонам на основе предоставленного контекста без дополнительного обучения. |
| PaletteNeRF: Palette-Based Appearance Editing of Neural Radiance Fields | 2023 | Chaoqun Kuang, Fujun Luan, Shenghua Bao, Zexiang Shu, Evangelos Kalogerakis | [link](http://openaccess.thecvf.com/content/CVPR2023/html/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.html) | Представлен метод редактирования внешнего вида 3D-сцен, визуализированных через NeRF, используя цветовые палитры, что позволяет интуитивно и интерактивно корректировать стиль.  |
| StyleDrop: Text-to-Image Generation in Any Style | 2023 | Kfir Y. Sohn, Nataniel Ruiz, Kalyan Vasudev Alwala, De-Cheng Chen, Irina Blok, Andrea Tagliasacchi | [link](https://arxiv.org/abs/2306.00983) | Введена система генерации изображений по текстовому описанию, способная создавать изображения в любом заданном стиле, расширяя возможности генеративных моделей в задачах переноса стиля. |
| Diff-Font: Diffusion Model for Robust One-Shot Font Generation | 2024 | Rui He, Xu Chen, Changyou Wang, Jingbo Liu, Baocai Yin, Dacheng Tao | [link](https://link.springer.com/article/10.1007/s11263-024-02137-0) | Предложена диффузионная модель для генерации новых стилей шрифтов из одного примера, улучшая устойчивость и разнообразие в переносе и генерации стилей шрифтов. |
| DiffFashion: Reference-Based Fashion Design with Structure-Aware Transfer by Diffusion Models | 2023 | Wensheng Cao, Wentao Chai, Shouhong Hao, Yijie Zhang, Feng Zheng | [link](https://ieeexplore.ieee.org/abstract/document/10261222/) | Представлен подход на основе диффузионных моделей для дизайна моды, позволяющий перенос стиля на основе референса с сохранением структурных деталей одежды. |
| TextStyleBrush: Transfer of Text Aesthetics from a Single Example | 2023 | Yossi Gandelsman, Yaniv Taigman, Yedid Hoshen | [link](https://ieeexplore.ieee.org/abstract/document/10027471/) | Введен метод переноса стиля и эстетики текста с использованием одного референсного изображения, облегчая приложения в персонализированном редактировании и синтезе текста. |
